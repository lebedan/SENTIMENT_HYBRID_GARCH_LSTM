{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #tensorflow warning hide\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "\n",
    "###################GPU CONFIG###################\n",
    "#gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "#tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "#tf.config.experimental.set_memory_growth(device=gpus[0], enable=True)\n",
    "###################GPU CONFIG###################\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.activations import elu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chi2\n",
    "import arch\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import ipynbname\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import vartests\n",
    "import csv\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy import optimize\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "##########R Packages##########\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.vectors import IntVector\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects import ListVector\n",
    "from rpy2.robjects.vectors import FloatVector\n",
    "from rpy2.robjects import numpy2ri, r\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.numpy2ri\n",
    "##########R Packages##########\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# If you specifically want to ignore warnings from pandas, you can do so as follows\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pandas')\n",
    "\n",
    "# Set options to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Optionally, set the display width to ensure that pandas does not wrap text\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set the column display length to maximum\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83becc07-2711-4979-ba0a-a15de04f6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################GPU CONFIG###################\n",
    "'''\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpu_number = 0 #### GPU number \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[gpu_number], 'GPU') \n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "'''\n",
    "###################GPU CONFIG###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########PARAMS##########\n",
    "garch_train_size=21*3 # in days\n",
    "train_size=0.7\n",
    "scaleing=True\n",
    "look_back=21\n",
    "confidence_level = 0.975\n",
    "patience=30\n",
    "validation_split=0.3\n",
    "num_iterations=3\n",
    "absolute=True #predict volatility instead of variance\n",
    "learning_rates=[0.001]\n",
    "batch_size=32\n",
    "epochs=200\n",
    "\n",
    "cwd = Path.cwd()\n",
    "news_lib='News_Daily_5Y'\n",
    "twitter_lib='Twitter_Daily_5Y'\n",
    "path_news=cwd/news_lib\n",
    "path_twitter=cwd/twitter_lib\n",
    "\n",
    "sp500_data = pd.read_excel(\"S&P500.xlsx\")\n",
    "sp500 = sp500_data[\"Ticker\"]\n",
    "sp500 = sp500.str.replace(\"/\", \"_\")\n",
    "##########PARAMS##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe06e69-44a5-4758-a1e4-4e4755d5bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realized_volatility_daily(series_log_return):\n",
    "    n = len(series_log_return)\n",
    "    return np.sqrt(np.sum(series_log_return**2)/(n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a26f3-aa84-43ba-b7f7-57596152b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def garch_data_generate(ticker,path_news,path_twitter):\n",
    "    def data_merge(ticker,path_news,path_twitter):\n",
    "        #News\n",
    "        files = os.listdir(path_news)\n",
    "        # Filter files that start with the specified prefix\n",
    "        matching_files = [file for file in files if file.startswith(ticker)]\n",
    "        #print(matching_files)\n",
    "        if len(matching_files) > 1:\n",
    "            print(\"There are more than\", threshold, \"files starting with the prefix\", prefix, \"in the directory.\")\n",
    "            return None\n",
    "        news_df = pd.read_csv(os.path.join(path_news, matching_files[0]), encoding='utf-8', index_col='Date', parse_dates=True)\n",
    "\n",
    "        #Twitter\n",
    "        files = os.listdir(path_twitter)\n",
    "        # Filter files that start with the specified prefix\n",
    "        matching_files = [file for file in files if file.startswith(ticker)]\n",
    "\n",
    "        if len(matching_files) > 1:\n",
    "            print(\"There are more than\", threshold, \"files starting with the prefix\", prefix, \"in the directory.\")\n",
    "            return None\n",
    "        twitter_df = pd.read_csv(os.path.join(path_twitter, matching_files[0]), encoding='utf-8', index_col='Date', parse_dates=True)\n",
    "\n",
    "        df = pd.merge(news_df,twitter_df, on=\"Date\",how='inner')\n",
    "        return df\n",
    "    def data_transformer(data):\n",
    "        data = data.sort_values(by='Date')\n",
    "        data[\"Date\"]=data.index\n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "        columns_to_drop = [\"(R1) Open_y\",\"(R1) High_y\",\"(R1) Low_y\",\"(R1) Close_y\"] \n",
    "        data = data.drop(columns=columns_to_drop)\n",
    "        columns_to_rename = {\"(R1) Open_x\": \"(R1) Open\",\"(R1) High_x\": \"(R1) High\",\"(R1) Low_x\":\"(R1) Low\",\"(R1) Close_x\":\"(R1) Close\"}  # Dictionary mapping old column names to new names\n",
    "        data = data.rename(columns=columns_to_rename)\n",
    "        \n",
    "        \n",
    "        new_columns = [col.replace(\" - Realtime\", \"\") for col in data.columns]\n",
    "\n",
    "        data.columns = new_columns\n",
    "        \n",
    "        for column in data.select_dtypes(include=['object']).columns:\n",
    "        # Replace \",\" with \".\" in the entire column and convert the column to float\n",
    "            if column != \"Date\":\n",
    "                data[column] = data[column].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "        #Make the modifications for the analysis\n",
    "        \n",
    "        data = data.dropna(subset=[\"(R1) Close\", \"News Positive Sentiment Count\", \"News Negative Sentiment Count\",\"News Publication Count (L1)\",\"Twitter Positive Sentiment Count\", \"Twitter Negative Sentiment Count\",\"Twitter Publication Count (L1)\"])\n",
    "\n",
    "        data['Log Returns'] = np.log(data['(R1) Close'] / data['(R1) Close'].shift(1))\n",
    "\n",
    "        data[\"News Positive Sentiment\"] = data[\"News Positive Sentiment Count\"]   / data[\"News Publication Count (L1)\"]\n",
    "        data[\"News Negative Sentiment\"] = -data[\"News Negative Sentiment Count\"]   / data[\"News Publication Count (L1)\"]\n",
    "        data[\"Twitter Positive Sentiment\"] = data[\"Twitter Positive Sentiment Count\"]   / data[\"Twitter Publication Count (L1)\"]\n",
    "        data[\"Twitter Negative Sentiment\"] = -data[\"Twitter Negative Sentiment Count\"]   / data[\"Twitter Publication Count (L1)\"]\n",
    "        data = data.dropna(subset=[\"Log Returns\"])\n",
    "        data = data.replace(np.inf, 0)\n",
    "        data = data.replace(np.nan, 0)\n",
    "        data=data[[\"Date\",\"Log Returns\",\"News Positive Sentiment\",\"News Negative Sentiment\",\"Twitter Positive Sentiment\",\"Twitter Negative Sentiment\"]]\n",
    "        data['Date'] = pd.to_datetime(data['Date'])  # Convert the date column to datetime format\n",
    "        data.set_index('Date', inplace=True)\n",
    "        return data\n",
    "    df=data_merge(ticker,path_news,path_twitter)\n",
    "    data=data_transformer(df)\n",
    "    return data\n",
    "    \n",
    "def generate_garch_forecast(data, window_size, forecast_horizon=1):\n",
    "    refit_every=1\n",
    "    model_fit = None\n",
    "    forecasts = []\n",
    "    dates = []\n",
    "\n",
    "    p = 1\n",
    "    q = 1\n",
    "\n",
    "\n",
    "    for i in range(window_size, len(data)):\n",
    "        window_data = data.iloc[:i]\n",
    "        \n",
    "        # Refit the model every 'refit_every' runs\n",
    "        if i % refit_every == 0 or model_fit is None:\n",
    "            j=0\n",
    "            model = arch_model(window_data, vol='GARCH', mean='Constant', p=p, q=q)\n",
    "            model_fit = model.fit(disp='off')\n",
    "            forecast = model_fit.forecast(horizon=refit_every)\n",
    "            forecasts.append(forecast.variance.values[-1][-1])\n",
    "            dates.append(data.index[i])\n",
    "\n",
    "        forecasts.append(forecast.variance.values[-1][0])\n",
    "        j+=1\n",
    "        dates.append(data.index[i])\n",
    "        \n",
    "    df = pd.DataFrame({'GARCH_Forecast': forecasts})    \n",
    "    dates_list = [pd.to_datetime(date_str) for date_str in dates]\n",
    "    df.set_index(pd.Index(dates_list), inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fcc3a-ab38-443b-98bb-a98d799fc3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_garch_x_forecast(data, window_size):\n",
    "\n",
    "    file_name='R_data.csv'\n",
    "    garch_data.to_csv(file_name, index=True)\n",
    "    file=str(cwd/file_name)\n",
    "\n",
    "    # Load the R script\n",
    "    with open(\"R_GARCH_X.R\", \"r\") as f:\n",
    "        r_code = f.read()\n",
    "    \n",
    "    # Evaluate the R code\n",
    "    robjects.r(r_code)\n",
    "    \n",
    "    # Call the R function with parameters\n",
    "    robjects.r['garch_x'](file, window_size)\n",
    "\n",
    "    df = pd.read_csv('Python_Input.csv', index_col=0)\n",
    "    df=df[[\"Sigma\"]]**2\n",
    "    df = df.rename(columns={'Sigma': 'GARCH_X_Forecast'})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc197d9-89ba-4ffa-b2f8-2d0813df73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(path_news,path_twitter,ticker,look_back,garch_train_size,train_size,scaleing,absolute,simple,garch_prediction):\n",
    "    \n",
    "    def data_merge(ticker,path_news,path_twitter):\n",
    "        #News\n",
    "        files = os.listdir(path_news)\n",
    "        # Filter files that start with the specified prefix\n",
    "        matching_files = [file for file in files if file.startswith(ticker)]\n",
    "        if len(matching_files) > 1:\n",
    "            print(\"There are more than\", threshold, \"files starting with the prefix\", prefix, \"in the directory.\")\n",
    "            return None\n",
    "        news_df = pd.read_csv(os.path.join(path_news, matching_files[0]), encoding='utf-8', index_col='Date', parse_dates=True)\n",
    "\n",
    "        #Twitter\n",
    "        files = os.listdir(path_twitter)\n",
    "        # Filter files that start with the specified prefix\n",
    "        matching_files = [file for file in files if file.startswith(ticker)]\n",
    "\n",
    "        if len(matching_files) > 1:\n",
    "            print(\"There are more than\", threshold, \"files starting with the prefix\", prefix, \"in the directory.\")\n",
    "            return None\n",
    "        twitter_df = pd.read_csv(os.path.join(path_twitter, matching_files[0]), encoding='utf-8', index_col='Date', parse_dates=True)\n",
    "\n",
    "        df = pd.merge(news_df,twitter_df, on=\"Date\",how='inner')\n",
    "        return df\n",
    "        \n",
    "    def data_transformer(data,absolute):\n",
    "        data = data.sort_values(by='Date')\n",
    "        data[\"Date\"]=data.index\n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "        columns_to_drop = [\"(R1) Open_y\",\"(R1) High_y\",\"(R1) Low_y\",\"(R1) Close_y\"] \n",
    "        data = data.drop(columns=columns_to_drop)\n",
    "        columns_to_rename = {\"(R1) Open_x\": \"(R1) Open\",\"(R1) High_x\": \"(R1) High\",\"(R1) Low_x\":\"(R1) Low\",\"(R1) Close_x\":\"(R1) Close\"}  # Dictionary mapping old column names to new names\n",
    "        data = data.rename(columns=columns_to_rename)\n",
    "        \n",
    "        \n",
    "        new_columns = [col.replace(\" - Realtime\", \"\") for col in data.columns]\n",
    "\n",
    "        data.columns = new_columns\n",
    "        \n",
    "        for column in data.select_dtypes(include=['object']).columns:\n",
    "        # Replace \",\" with \".\" in the entire column and convert the column to float\n",
    "            if column != \"Date\":\n",
    "                data[column] = data[column].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "        #Make the modifications for the analysis\n",
    "        data = data.dropna(subset=[\"(R1) Close\", \"News Positive Sentiment Count\", \"News Negative Sentiment Count\",\"News Publication Count (L1)\",\"Twitter Positive Sentiment Count\", \"Twitter Negative Sentiment Count\",\"Twitter Publication Count (L1)\"])\n",
    "\n",
    "        data['Log Returns'] = np.log(data['(R1) Close'] / data['(R1) Close'].shift(1))\n",
    "        if absolute==True:\n",
    "            data['Log Returns Variance']  = data['Log Returns'].rolling(window=10).apply(realized_volatility_daily)\n",
    "        else:\n",
    "            data['Log Returns Variance']  = data['Log Returns'].rolling(window=10).apply(realized_variance_daily)\n",
    "\n",
    "        data[\"News Positive Sentiment\"] = data[\"News Positive Sentiment Count\"]   / data[\"News Publication Count (L1)\"]\n",
    "        data[\"News Negative Sentiment\"] = -data[\"News Negative Sentiment Count\"]   / data[\"News Publication Count (L1)\"]\n",
    "        data[\"Twitter Positive Sentiment\"] = data[\"Twitter Positive Sentiment Count\"]   / data[\"Twitter Publication Count (L1)\"]\n",
    "        data[\"Twitter Negative Sentiment\"] = -data[\"Twitter Negative Sentiment Count\"]   / data[\"Twitter Publication Count (L1)\"]\n",
    "        data = data.replace(np.inf, 0)\n",
    "        data = data.dropna(subset=[\"News Positive Sentiment\",\"News Negative Sentiment\",\"Twitter Positive Sentiment\",\"Twitter Negative Sentiment\", \"Log Returns Variance\"])\n",
    "        return data\n",
    "        \n",
    "    def data_prepare(data,time_steps,train_size,scaleing):\n",
    "        columns=[\"Log Returns Variance\",\"GARCH_Forecast\",\"News Positive Sentiment\",\"News Negative Sentiment\",\"Twitter Positive Sentiment\",\n",
    "                    \"Twitter Negative Sentiment\"]\n",
    "        data=data[columns]\n",
    "        cut=round(train_size*data.shape[0])\n",
    "        \n",
    "        if scaleing==True:\n",
    "\n",
    "            columns=[\"Log Returns Variance\"]\n",
    "            data_y=data[columns]\n",
    "            scaler = StandardScaler()\n",
    "            data_y= scaler.fit_transform(data_y.values.reshape(-1, 1))\n",
    "\n",
    "            columns=[\"Log Returns Variance\",\"GARCH_Forecast\",\"News Positive Sentiment\",\"News Negative Sentiment\",\"Twitter Positive Sentiment\",\n",
    "                     \"Twitter Negative Sentiment\"]\n",
    "            data_x=data[columns]\n",
    "            scaler1 = StandardScaler()\n",
    "            data_x= scaler1.fit_transform(data_x.values)\n",
    "\n",
    "            data_scaled= np.concatenate((data_y, data_x), axis=1)\n",
    "        else:\n",
    "            scaler=None\n",
    "            columns=[\"Log Returns Variance\",\"GARCH_Forecast\",\"News Positive Sentiment\",\"News Negative Sentiment\",\"Twitter Positive Sentiment\",\n",
    "                     \"Twitter Negative Sentiment\"]\n",
    "            data=data[columns]\n",
    "            data_scaled=data.values\n",
    "\n",
    "        cut=round(train_size*data.shape[0])\n",
    "        training_set_scaled = data_scaled[:cut]\n",
    "        test_set_scaled = data_scaled[cut:]\n",
    "\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for i in range(time_steps, len(training_set_scaled)-time_steps-1):\n",
    "            X_train.append(training_set_scaled[i-time_steps:i, :])\n",
    "            y_train.append(training_set_scaled[i, 0])\n",
    "        X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], len(training_set_scaled[0])))\n",
    "\n",
    "        inputs = data_scaled[len(data_scaled) - len(test_set_scaled) - time_steps:]\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for i in range(time_steps,time_steps+len(test_set_scaled)):\n",
    "            X_test.append(inputs[i-time_steps:i,:])\n",
    "            y_test.append(inputs[i-time_steps:i,0])\n",
    "        X_test,y_test= np.array(X_test),np.array(y_test)\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], len(test_set_scaled[0])))\n",
    "\n",
    "        return X_train, X_test, y_train, y_test,scaler\n",
    "        \n",
    "    def data_prepare_simple(data,time_steps,train_size,scaleing):\n",
    "\n",
    "        columns=[\"Log Returns Variance\",\"GARCH_Forecast\"]\n",
    "        data=data[columns]\n",
    "        cut=round(train_size*data.shape[0])\n",
    "        if scaleing==True:\n",
    "\n",
    "            columns=[\"Log Returns Variance\"]\n",
    "            data_y=data[columns]\n",
    "            scaler = StandardScaler()\n",
    "            data_y= scaler.fit_transform(data_y.values.reshape(-1, 1))\n",
    "\n",
    "            columns=[\"Log Returns Variance\",\"GARCH_Forecast\"]\n",
    "            data_x=data[columns]\n",
    "            scaler1 = StandardScaler()\n",
    "            data_x= scaler1.fit_transform(data_x.values)\n",
    "\n",
    "            data_scaled= np.concatenate((data_y, data_x), axis=1)\n",
    "        else:\n",
    "            scaler=None\n",
    "            columns=[\"Log Returns Variance\",\"GARCH_Forecast\"]\n",
    "            data=data[columns]\n",
    "            data_scaled=data.values\n",
    "\n",
    "        cut=round(train_size*data.shape[0])\n",
    "        training_set_scaled = data_scaled[:cut]\n",
    "        test_set_scaled = data_scaled[cut:]\n",
    "\n",
    "\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for i in range(time_steps, len(training_set_scaled)-time_steps-1):\n",
    "            X_train.append(training_set_scaled[i-time_steps:i, :])\n",
    "            y_train.append(training_set_scaled[i, 0])\n",
    "        X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], len(training_set_scaled[0])))\n",
    "\n",
    "\n",
    "        inputs = data_scaled[len(data_scaled) - len(test_set_scaled) - time_steps:]\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for i in range(time_steps,time_steps+len(test_set_scaled)):\n",
    "            X_test.append(inputs[i-time_steps:i,:])\n",
    "            y_test.append(inputs[i-time_steps:i,0])\n",
    "        X_test,y_test= np.array(X_test),np.array(y_test)\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], len(test_set_scaled[0])))\n",
    "\n",
    "        return X_train, X_test, y_train, y_test,scaler\n",
    "\n",
    "    df=data_merge(ticker,path_news,path_twitter)\n",
    "    data=data_transformer(df,absolute)\n",
    "\n",
    "    ##################MERGE DATA AND GARCH PREDICTIONS##############################\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data.set_index(data[\"Date\"], inplace=True)\n",
    "    garch_prediction['GARCH_Forecast'] = garch_prediction['GARCH_Forecast'].shift(-1)\n",
    "    garch_prediction['GARCH_Forecast'].fillna(method='ffill', inplace=True)\n",
    "    data = pd.merge(data,garch_prediction,left_index=True, right_index=True, how='inner')\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    ##################MERGE DATA AND GARCH PREDICTIONS##############################\n",
    "    \n",
    "    cut = round(len(data) * train_size)\n",
    "    train_data = data[:cut]\n",
    "    test_data = data[cut:]\n",
    "    if simple==True:\n",
    "        X_train, X_test, y_train, y_test,scaler=data_prepare_simple(data,look_back,train_size,scaleing)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test,scaler=data_prepare(data,look_back,train_size,scaleing)\n",
    "    return X_train, y_train, X_test, y_test,train_data,test_data,data,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153dd3d-b74f-47bc-8173-3a98c0f8836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://github.com/BayerSe/VaR-Backtesting\n",
    "def duration_test(\n",
    "    violations: Union[List[int], np.ndarray, pd.Series, pd.DataFrame],\n",
    "    conf_level: float = 0.95,\n",
    ") -> Dict:\n",
    "\n",
    "    \"\"\"Perform the Christoffersen and Pelletier Test (2004) called Duration Test.\n",
    "        The main objective is to know if the VaR model responds quickly to market movements\n",
    "         in order to do not form volatility clusters.\n",
    "        Duration is time betwenn violations of VaR.\n",
    "        This test verifies if violations has no memory i.e. should be independent.\n",
    "\n",
    "        Parameters:\n",
    "            violations (series): series of violations of VaR\n",
    "            conf_level (float):  test confidence level\n",
    "        Returns:\n",
    "            answer (dict):       statistics and decision of the test\n",
    "    \"\"\"\n",
    "    typeok = False\n",
    "    if isinstance(violations, pd.core.series.Series) or isinstance(\n",
    "        violations, pd.core.frame.DataFrame\n",
    "    ):\n",
    "        violations = violations.values.flatten()\n",
    "        typeok = True\n",
    "    elif isinstance(violations, np.ndarray):\n",
    "        violations = violations.flatten()\n",
    "        typeok = True\n",
    "    elif isinstance(violations, list):\n",
    "        typeok = True\n",
    "    if not typeok:\n",
    "        raise ValueError(\"Input must be list, array, series or dataframe.\")\n",
    "\n",
    "    N = int(sum(violations))\n",
    "    first_hit = violations[0]\n",
    "    last_hit = violations[-1]\n",
    "\n",
    "    duration = [i + 1 for i, x in enumerate(violations) if x == 1]\n",
    "\n",
    "    D = np.diff(duration)\n",
    "\n",
    "    TN = len(violations)\n",
    "    C = np.zeros(len(D))\n",
    "\n",
    "    if not duration or (D.shape[0] == 0 and len(duration) == 0):\n",
    "        duration = [0]\n",
    "        D = [0]\n",
    "        N = 1\n",
    "\n",
    "    if first_hit == 0:\n",
    "        C = np.append(1, C)\n",
    "        D = np.append(duration[0], D)  # days until first violation\n",
    "\n",
    "    if last_hit == 0:\n",
    "        C = np.append(C, 1)\n",
    "        D = np.append(D, TN - duration[-1])\n",
    "\n",
    "    else:\n",
    "        N = len(D)\n",
    "\n",
    "    def likDurationW(x, D, C, N):\n",
    "        b = x\n",
    "        a = ((N - C[0] - C[N - 1]) / (sum(D ** b))) ** (1 / b)\n",
    "        lik = (\n",
    "            C[0] * np.log(pweibull(D[0], a, b, survival=True))\n",
    "            + (1 - C[0]) * dweibull(D[0], a, b, log=True)\n",
    "            + sum(dweibull(D[1 : (N - 1)], a, b, log=True))\n",
    "            + C[N - 1] * np.log(pweibull(D[N - 1], a, b, survival=True))\n",
    "            + (1 - C[N - 1]) * dweibull(D[N - 1], a, b, log=True)\n",
    "        )\n",
    "\n",
    "        if np.isnan(lik) or np.isinf(lik):\n",
    "            lik = np.float64(1e10)\n",
    "        else:\n",
    "            lik = -lik\n",
    "        return lik\n",
    "\n",
    "    # When b=1 we get the exponential\n",
    "    def dweibull(D, a, b, log=False):\n",
    "        # density of Weibull\n",
    "        pdf = b * np.log(a) + np.log(b) + (b - 1) * np.log(D) - (a * D) ** b\n",
    "        if not log:\n",
    "            pdf = np.exp(pdf)\n",
    "        return pdf\n",
    "\n",
    "    def pweibull(D, a, b, survival=False):\n",
    "        # distribution of Weibull\n",
    "        cdf = 1 - np.exp(-((a * D) ** b))\n",
    "        if survival:\n",
    "            cdf = 1 - cdf\n",
    "        return cdf\n",
    "\n",
    "    optimizedBetas = optimize.minimize(\n",
    "        likDurationW, x0=[2], args=(D, C, N), method=\"L-BFGS-B\", bounds=[(0.001, 10)]\n",
    "    )\n",
    "\n",
    "    print(optimizedBetas.message)\n",
    "\n",
    "    b = optimizedBetas.x\n",
    "    uLL = -likDurationW(b, D, C, N)\n",
    "    rLL = -likDurationW(np.array([1]), D, C, N)\n",
    "    LR = 2 * (uLL - rLL)\n",
    "    LRp = 1 - chi2.cdf(LR, 1)\n",
    "\n",
    "    H0 = \"Duration Between Exceedances have no memory (Weibull b=1 = Exponential)\"\n",
    "    # i.e. whether we fail to reject the alternative in the LR test that b=1 (hence correct model)\n",
    "    if LRp < (1 - conf_level):\n",
    "        decision = \"Reject H0\"\n",
    "    else:\n",
    "        decision = \"Fail to Reject H0\"\n",
    "\n",
    "    answer = {\n",
    "        \"weibull exponential\": b,\n",
    "        \"unrestricted log-likelihood\": uLL,\n",
    "        \"restricted log-likelihood\": rLL,\n",
    "        \"log-likelihood\": LR,\n",
    "        \"log-likelihood ratio test statistic\": LRp,\n",
    "        \"null hypothesis\": H0,\n",
    "        \"decision\": decision,\n",
    "    }\n",
    "\n",
    "    return answer\n",
    "\n",
    "def smooth_loss(actual,forecast,alpha,delta=25, return_mean=True):\n",
    "    \"\"\"Gonzalez-Rivera, Lee and Mishra (2004)\"\"\"\n",
    "    loss = ((alpha - (1 + np.exp(delta*(actual - forecast)))**-1) * (actual - forecast))\n",
    "    if return_mean:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "def quadratic_loss(actual,forecast,hit_series,return_mean=True):\n",
    "    \"\"\"Lopez (1999); Martens et al. (2009)\"\"\"\n",
    "    loss = (hit_series* (1 + (actual - forecast)**2))\n",
    "    if return_mean:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss\n",
    "def firm_loss(actual,forecast,hit_series, c=1, return_mean=True):\n",
    "    \"\"\"Sarma et al. (2003)\"\"\"\n",
    "    loss = (hit_series * (1 + (actual - forecast)**2) - c*(1-hit_series) * forecast)\n",
    "    if return_mean:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e708320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    loss = K.sqrt(K.mean(K.square((y_true - y_pred) / y_true)))\n",
    "    return loss\n",
    "\n",
    "def nn_model_predict(X_train,y_train,X_test,y_test,scaler,scaling,patience,validation_split,num_iterations,y_true,learning_rates,batch_size,epochs):\n",
    "\n",
    "    initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "    z = L.Input(shape=(X_train.shape[1],X_train.shape[2]), name=\"Input\")\n",
    "    y = L.Bidirectional(L.LSTM(128,activation=\"tanh\",return_sequences=True,name=\"LSTM1\",kernel_initializer=initializer))(z) #,kernel_regularizer=regularizers.l1(0.01)\n",
    "    y = L.Dropout(0.1)(y)\n",
    "    y = (L.LSTM(64,activation=\"tanh\",return_sequences=True,name=\"LSTM2\",kernel_initializer=initializer))(y)\n",
    "    y = L.Dropout(0.1)(y)\n",
    "    y = (L.LSTM(32,activation=\"tanh\",return_sequences=True,name=\"LSTM3\",kernel_initializer=initializer))(y)\n",
    "    y = L.Dropout(0.1)(y)\n",
    "    y = (L.LSTM(16,activation=\"tanh\",return_sequences=False,name=\"LSTM5\",kernel_initializer=initializer))(y)\n",
    "    y = L.Dropout(0.1)(y)\n",
    "    x = L.Dense(1, activation=\"linear\", name=\"o1\",kernel_initializer=initializer)(y)\n",
    "\n",
    "    model = M.Model(z, x, name=\"DNN\")\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_mse = float('inf')\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        for i in range(num_iterations):            \n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, tf.keras.layers.LSTM):\n",
    "                    layer.set_weights([tf.random.normal(w.shape) for w in layer.get_weights()])\n",
    "\n",
    "            adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) #\n",
    "            model.compile(optimizer=adam_optimizer, loss='mse', metrics=[rmspe]) #mse\n",
    "            earlyStop=EarlyStopping(monitor='val_rmspe',verbose=2,mode='min',patience=patience,restore_best_weights=True)\n",
    "            history=model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0,validation_split=validation_split,callbacks=[earlyStop])\n",
    "            mse=earlyStop.best\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_model = model\n",
    "    predict_nn= best_model.predict(X_test)\n",
    "    predict_nn=predict_nn[:,0]\n",
    "    if scaleing==True:\n",
    "        predict_nn = scaler.inverse_transform(predict_nn.reshape(-1,1))\n",
    "        \n",
    "    return predict_nn\n",
    "def nn_model_predict_simple(X_train,y_train,X_test,y_test,scaler,scaling,patience,validation_split,num_iterations,y_true,learning_rates,batch_size,epochs):\n",
    "\n",
    "    initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "    z = L.Input(shape=(X_train.shape[1],X_train.shape[2]), name=\"Input\")\n",
    "    y = L.Bidirectional(L.LSTM(128,activation=\"tanh\",return_sequences=True,name=\"LSTM1\",kernel_initializer=initializer))(z) #,kernel_regularizer=regularizers.l1(0.01)\n",
    "    y = L.Dropout(0.1)(y)\n",
    "    y = (L.LSTM(64,activation=\"tanh\",return_sequences=True,name=\"LSTM2\",kernel_initializer=initializer))(y)\n",
    "    y = L.Dropout(0.1)(y)\n",
    "    y = (L.LSTM(32,activation=\"tanh\",return_sequences=True,name=\"LSTM3\",kernel_initializer=initializer))(y)\n",
    "    y = L.Dropout(0.1)(y)\n",
    "    y = (L.LSTM(16,activation=\"tanh\",return_sequences=False,name=\"LSTM5\",kernel_initializer=initializer))(y)\n",
    "    y = L.Dropout(0.1)(y)\n",
    "    x = L.Dense(1, activation=\"linear\", name=\"o1\",kernel_initializer=initializer)(y)\n",
    "    \n",
    "    model = M.Model(z, x, name=\"DNN\")\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_mse = float('inf')\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        for i in range(num_iterations):            \n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, tf.keras.layers.LSTM):\n",
    "                    layer.set_weights([tf.random.normal(w.shape) for w in layer.get_weights()])\n",
    "\n",
    "            adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) #\n",
    "            model.compile(optimizer=adam_optimizer, loss='mse', metrics=[rmspe])\n",
    "            earlyStop=EarlyStopping(monitor=\"val_rmspe\",verbose=2,mode='min',patience=patience,restore_best_weights=True)\n",
    "            history=model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0,validation_split=validation_split,callbacks=[earlyStop])\n",
    "            mse=earlyStop.best\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_model = model\n",
    "    predict_nn= best_model.predict(X_test)\n",
    "    predict_nn=predict_nn[:,0]\n",
    "    if scaleing==True:\n",
    "        predict_nn = scaler.inverse_transform(predict_nn.reshape(-1,1))\n",
    "        \n",
    "    return predict_nn\n",
    "def nn_model_predict_saved(X_train,y_train,X_test,y_test,scaler,scaling,patience,validation_split,num_iterations,y_true,learning_rates,batch_size,epochs):\n",
    "    \n",
    "    initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "    saved_model = tf.keras.models.load_model('best_best_7.keras')\n",
    "\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(batch_size=batch_size,shape=(X_train.shape[1],X_train.shape[2]), name=\"Input\"))\n",
    "    for layer in saved_model.layers[1:]:\n",
    "        model.add(layer)\n",
    "\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_mse = float('inf')\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        for i in range(num_iterations):            \n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, tf.keras.layers.LSTM):\n",
    "                    layer.set_weights([tf.random.normal(w.shape) for w in layer.get_weights()])\n",
    "\n",
    "            adam_optimizer = tf.keras.optimizers.Adam() #learning_rate=learning_rate\n",
    "            model.compile(optimizer=adam_optimizer, loss='mse', metrics=['mse'])\n",
    "            earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=patience,restore_best_weights=True)\n",
    "            history=model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0,validation_split=validation_split,callbacks=[earlyStop])\n",
    "            stopped_epoch = earlyStop.stopped_epoch\n",
    "            mse = min(history.history['val_loss'][:stopped_epoch])\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_model = model\n",
    "    predict_nn= best_model.predict(X_test)\n",
    "    predict_nn=predict_nn[:,0]\n",
    "    if scaleing==True:\n",
    "        predict_nn = scaler.inverse_transform(predict_nn.reshape(-1,1))\n",
    "    return predict_nn   \n",
    "\n",
    "def calculate_var(daily_variance,daily_mean, confidence_level=0.95):\n",
    "    standard_deviation = np.sqrt(daily_variance)\n",
    "    z_score = -1.0 * np.abs(norm.ppf((1 - confidence_level) / 2))\n",
    "    var = daily_mean + z_score * standard_deviation \n",
    "    return var\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    output = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return output\n",
    "    \n",
    "def Losses(y_true,y_pred):\n",
    "    mse=mean_squared_error(y_true, y_pred)\n",
    "    mae=mean_absolute_error(y_true, y_pred)\n",
    "    rmse=rmspe_metric(y_true, y_pred)\n",
    "    return mse,mae,rmse\n",
    "def Violation_ratio(returns,predictions,confidence_level):\n",
    "    p=1-confidence_level\n",
    "    VR = sum(VR < predictions)/(p*(len(predictions)))\n",
    "    return VR\n",
    "\n",
    "def Joint_test(violations,var_conf_level,conf_level):\n",
    "    pof=vartests.kupiec_test(violations, var_conf_level=var_conf_level, conf_level=conf_level).get(\"log-likelihood\")\n",
    "    if np.isnan(pof) or np.isinf(pof):\n",
    "        pof = np.float(0.0)\n",
    "    ch=duration_test(violations, conf_level=0.95).get('log-likelihood')\n",
    "    if not isinstance(ch, np.ndarray):\n",
    "        ch=ch\n",
    "    else:\n",
    "        ch=ch[0]\n",
    "    joint=pof+ch\n",
    "    critical_chi_square = chi2.ppf(conf_level, 2)  # two degree of freedom\n",
    "    if joint> critical_chi_square:\n",
    "        result = \"Reject H0\"\n",
    "    else:\n",
    "        result = \"Fail to reject H0\"\n",
    "    return result\n",
    "\n",
    "def Violation_test(returns,predictions,confidence_level,plot=True):\n",
    "    p=1-confidence_level\n",
    "    results=[]\n",
    "    for col in range(predictions.shape[0]):\n",
    "        VR = sum(returns['Log Returns'].values < predictions[col])/(p*(len(predictions[col])))\n",
    "        print (col, \"\\n\",\n",
    "           \"Violation ratio:\", round(VR, 3))\n",
    "        results.append(VR)\n",
    "    if plot==True:\n",
    "        plt.plot(returns['Date'],returns['Log Returns'], color = 'red', label = \"Log_returns\")\n",
    "        plt.plot(returns['Date'],predictions[0], color = 'blue', label = \"NN_Social_Media_Prediction\")\n",
    "        plt.plot(returns['Date'],predictions[1], color = 'yellow', label = \"NN_Prediction\")\n",
    "        plt.plot(returns['Date'],predictions[2], color = 'green', label = \"GARCH_Prediction\")\n",
    "        plt.plot(returns['Date'],predictions[3], color = 'pink', label = \"GARCH_X_Prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return results\n",
    "def write_to_csv_last_line(csv_file_path, new_data):\n",
    "    # Open the CSV file in append mode with newline='' to handle new line characters correctly\n",
    "    with open(csv_file_path, 'a', newline='') as csv_file:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write data into the last new line\n",
    "        csv_writer.writerow(new_data)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture --no-display\n",
    "start_time = time.time()\n",
    "\n",
    "nb_fname = ipynbname.name()\n",
    "csv_name=nb_fname+\"_results.csv\"\n",
    "\n",
    "for ticker in sp500:\n",
    "    try:\n",
    "        print(f\"{ticker} loaded successfully.\")\n",
    "\n",
    "        #simple GARCH(1,1)\n",
    "\n",
    "        garch_data=garch_data_generate(ticker,path_news,path_twitter)\n",
    "        garch_prediction=generate_garch_forecast(garch_data[[\"Log Returns\"]], garch_train_size)\n",
    "\n",
    "        #GARCH_X\n",
    "        garch_x_prediction=generate_garch_x_forecast(garch_data, garch_train_size)\n",
    "\n",
    "\n",
    "        if absolute==True:\n",
    "            garch_prediction=garch_prediction**(1/2)\n",
    "            garch_x_prediction=garch_x_prediction**(1/2)\n",
    "            \n",
    "        #social media neural network\n",
    "        X_train, y_train, X_test, y_test,train_data,test_data,data,scaler=data_generator(path_news,path_twitter,ticker,look_back,garch_train_size,train_size,scaleing,absolute,False,garch_prediction)\n",
    "        \n",
    "        #GET EXPECTED VALUE, LOG RETURNS AND REAL VARIANCE\n",
    "        daily_mean=test_data['Log Returns'].mean()\n",
    "        returns=test_data[['Date','Log Returns']]\n",
    "        y_true=test_data['Log Returns Variance'].values\n",
    "    \n",
    "        nn_prediction_social_media=nn_model_predict(X_train, y_train, X_test, y_test,scaler,scaleing,patience,validation_split,num_iterations,y_true,learning_rates,batch_size,epochs)\n",
    "        nn_prediction_social_media=nn_prediction_social_media\n",
    "    \n",
    "        #simple GARCH-net\n",
    "        X_train, y_train, X_test, y_test,train_data,test_data,full_data,scaler=data_generator(path_news,path_twitter,ticker,look_back,garch_train_size,train_size,scaleing,absolute,True,garch_prediction)\n",
    "        #GET EXPECTED VALUE, LOG RETURNS AND REAL VARIANCE\n",
    "        daily_mean=test_data['Log Returns'].mean()\n",
    "        returns=test_data[['Date','Log Returns']]\n",
    "        y_true=test_data['Log Returns Variance']\n",
    "        nn_prediction=nn_model_predict_simple(X_train, y_train, X_test, y_test,scaler,scaleing,patience,validation_split,num_iterations,y_true,learning_rates,batch_size,epochs)\n",
    "        nn_prediction=nn_prediction\n",
    "    \n",
    "        #TRANSFORM BACK PREDICTIONS\n",
    "        if absolute==True:\n",
    "            garch_prediction=garch_prediction**2\n",
    "            garch_x_prediction=garch_x_prediction**2\n",
    "            nn_prediction_social_media=nn_prediction_social_media**2\n",
    "            nn_prediction=nn_prediction**2\n",
    "\n",
    "        #CALCULATE LOSSES\n",
    "        GARCH_prediction=garch_prediction[-len(nn_prediction):]\n",
    "        GARCH_prediction=GARCH_prediction['GARCH_Forecast'].values\n",
    "        mse_GARCH,mae_GARCH,rmspe_GARCH=Losses(y_true,GARCH_prediction)\n",
    "        GARCH_X_prediction=garch_x_prediction[-len(nn_prediction):]\n",
    "        GARCH_X_prediction=GARCH_X_prediction['GARCH_X_Forecast'].values\n",
    "        mse_GARCH_X,mae_GARCH_X,rmspe_GARCH_X=Losses(y_true,GARCH_X_prediction)\n",
    "        mse_nn_social_media,mae_nn_social_media,rmspe_nn_social_media=Losses(y_true,np.ravel(nn_prediction_social_media))\n",
    "        mse_nn,mae_nn,rmspe_nn=Losses(y_true,np.ravel(nn_prediction))\n",
    "    \n",
    "        del  X_train, y_train, X_test, y_test,train_data,test_data,full_data,scaler\n",
    "\n",
    "        #CALCUALTE VaR\n",
    "        nn_social_media_VAR=calculate_var(nn_prediction_social_media,daily_mean, confidence_level=confidence_level)\n",
    "        nn_VAR=calculate_var(nn_prediction,daily_mean, confidence_level=confidence_level)\n",
    "        GARCH_VAR=calculate_var(GARCH_prediction,daily_mean, confidence_level=confidence_level)\n",
    "        GARCH_X_VAR=calculate_var(GARCH_X_prediction,daily_mean, confidence_level=confidence_level)\n",
    "    \n",
    "        #CALCULATE LOSSES FROM VAR\n",
    "        ret=returns['Log Returns'].values\n",
    "        mse_GARCH_1,mae_GARCH_1,rmspe_GARCH_1=Losses(ret,GARCH_VAR)\n",
    "        mse_GARCH_X_1,mae_GARCH_X_1,rmspe_GARCH_X_1=Losses(ret,GARCH_X_VAR)\n",
    "        mse_nn_social_media_1,mae_nn_social_media_1,rmspe_nn_social_media_1=Losses(ret,nn_social_media_VAR)\n",
    "        mse_nn_1,mae_nn_1,rmspe_nn_1=Losses(ret,nn_VAR)\n",
    "    \n",
    "        #calculate violation ratios\n",
    "        predictions = np.vstack((nn_social_media_VAR.T, nn_VAR.T,GARCH_VAR.T,GARCH_X_VAR.T))\n",
    "        test=Violation_test(returns,predictions,confidence_level,True)\n",
    "        \n",
    "        violations=pd.DataFrame(nn_social_media_VAR,columns=['VAR'])\n",
    "        violations['Log Returns']=returns['Log Returns'].values\n",
    "        violations['violations'] = (violations['Log Returns'] <violations['VAR']).astype(int)\n",
    "        nn_social_media_Kupiec=vartests.kupiec_test(violations['violations'], var_conf_level=confidence_level, conf_level=0.95).get(\"result\")\n",
    "        nn_social_media_Christoffersen=duration_test(violations['violations'], conf_level=0.95).get('log-likelihood ratio test statistic')\n",
    "        if not isinstance(nn_social_media_Christoffersen, np.ndarray):\n",
    "            nn_social_media_Christoffersen=\"Reject H0\"\n",
    "        elif nn_social_media_Christoffersen[0]>=0.05:\n",
    "            nn_social_media_Christoffersen=\"Fail to reject H0\"\n",
    "        else:\n",
    "            nn_social_media_Christoffersen=\"Reject H0\"\n",
    "        nn_social_media_Joint=Joint_test(violations['violations'],confidence_level,0.95)\n",
    "        nn_social_media_QL=quadratic_loss(violations['Log Returns'],violations['VAR'],violations['violations'])\n",
    "        nn_social_media_FL=firm_loss(violations['Log Returns'],violations['VAR'],violations['violations'])\n",
    "        nn_social_media_SL=smooth_loss(violations['Log Returns'],violations['VAR'],1-confidence_level)\n",
    "            \n",
    "        violations=pd.DataFrame(nn_VAR,columns=['VAR'])\n",
    "        violations['Log Returns']=returns['Log Returns'].values\n",
    "        violations['violations'] = (violations['Log Returns'] <violations['VAR']).astype(int)\n",
    "        nn_Kupiec=vartests.kupiec_test(violations['violations'], var_conf_level=confidence_level, conf_level=0.95).get(\"result\")\n",
    "        nn_Christoffersen=duration_test(violations['violations'], conf_level=0.95).get('log-likelihood ratio test statistic')\n",
    "        if not isinstance(nn_Christoffersen, np.ndarray):\n",
    "            nn_Christoffersen=\"Reject H0\"\n",
    "        elif nn_Christoffersen[0]>=0.05:\n",
    "            nn_Christoffersen=\"Fail to reject H0\"\n",
    "        else:\n",
    "            nn_Christoffersen=\"Reject H0\"\n",
    "        nn_Joint=Joint_test(violations['violations'],confidence_level,0.95)\n",
    "        nn_QL=quadratic_loss(violations['Log Returns'],violations['VAR'],violations['violations'])\n",
    "        nn_FL=firm_loss(violations['Log Returns'],violations['VAR'],violations['violations'])\n",
    "        nn_SL=smooth_loss(violations['Log Returns'],violations['VAR'],1-confidence_level)\n",
    "        \n",
    "        violations=pd.DataFrame(GARCH_VAR,columns=['VAR'])\n",
    "        violations['Log Returns']=returns['Log Returns'].values\n",
    "        violations['violations'] = (violations['Log Returns'] < violations['VAR']).astype(int)\n",
    "        GARCH_Kupiec=vartests.kupiec_test(violations['violations'], var_conf_level=confidence_level, conf_level=0.95).get(\"result\")\n",
    "        GARCH_Christoffersen=duration_test(violations['violations'], conf_level=0.95).get('log-likelihood ratio test statistic')\n",
    "        if not isinstance(GARCH_Christoffersen, np.ndarray):\n",
    "            GARCH_Christoffersen=\"Reject H0\"\n",
    "        elif GARCH_Christoffersen[0]>=0.05:\n",
    "            GARCH_Christoffersen=\"Fail to reject H0\"\n",
    "        else:\n",
    "            GARCH_Christoffersen=\"Reject H0\"\n",
    "        GARCH_Joint=Joint_test(violations['violations'],confidence_level,0.95)\n",
    "        GARCH_QL=quadratic_loss(violations['Log Returns'],violations['VAR'],violations['violations'])\n",
    "        GARCH_FL=firm_loss(violations['Log Returns'],violations['VAR'],violations['violations'])\n",
    "        GARCH_SL=smooth_loss(violations['Log Returns'],violations['VAR'],1-confidence_level)\n",
    "            \n",
    "        violations=pd.DataFrame(GARCH_X_VAR,columns=['VAR'])\n",
    "        violations['Log Returns']=returns['Log Returns'].values\n",
    "        violations['violations'] = (violations['Log Returns'] < violations['VAR']).astype(int)\n",
    "        GARCH_X_Kupiec=vartests.kupiec_test(violations['violations'], var_conf_level=confidence_level, conf_level=0.95).get(\"result\")\n",
    "        GARCH_X_Christoffersen=duration_test(violations['violations'], conf_level=0.95).get('log-likelihood ratio test statistic')\n",
    "        if not isinstance(GARCH_X_Christoffersen, np.ndarray):\n",
    "            GARCH_X_Christoffersen=\"Reject H0\"\n",
    "        elif GARCH_X_Christoffersen[0]>=0.05:\n",
    "            GARCH_X_Christoffersen=\"Fail to reject H0\"\n",
    "        else:\n",
    "            GARCH_X_Christoffersen=\"Reject H0\"\n",
    "        GARCH_X_Joint=Joint_test(violations['violations'],confidence_level,0.95)\n",
    "        GARCH_X_QL=quadratic_loss(violations['Log Returns'],violations['VAR'],violations['violations'])\n",
    "        GARCH_X_FL=firm_loss(violations['Log Returns'],violations['VAR'],violations['violations'])\n",
    "        GARCH_X_SL=smooth_loss(violations['Log Returns'],violations['VAR'],1-confidence_level)\n",
    "        \n",
    "        row=[ticker,mse_nn_social_media,rmspe_nn_social_media,mae_nn_social_media,mse_nn_social_media_1,rmspe_nn_social_media_1,\n",
    "             mae_nn_social_media_1,test[0],nn_social_media_Kupiec,nn_social_media_Christoffersen,nn_social_media_Joint,\n",
    "             nn_social_media_QL,nn_social_media_FL,nn_social_media_SL,mse_nn,rmspe_nn,mae_nn,mse_nn_1,rmspe_nn_1,\n",
    "             mae_nn_1,test[1],nn_Kupiec,nn_Christoffersen,nn_Joint,nn_QL,nn_FL,nn_SL,mse_GARCH,rmspe_GARCH,mae_GARCH,mse_GARCH_1,\n",
    "             rmspe_GARCH_1,mae_GARCH_1,test[2],GARCH_Kupiec,GARCH_Christoffersen,GARCH_Joint,GARCH_QL,GARCH_FL,GARCH_SL,\n",
    "             mse_GARCH_X,rmspe_GARCH_X,mae_GARCH_X,mse_GARCH_X_1,rmspe_GARCH_X_1,mae_GARCH_X_1,test[3],GARCH_X_Kupiec,\n",
    "             GARCH_X_Christoffersen,GARCH_X_Joint,GARCH_X_QL,GARCH_X_FL,GARCH_X_SL]\n",
    "\n",
    "        print(row)\n",
    "        write_to_csv_last_line(csv_name,row)\n",
    "    except Exception as e:\n",
    "        row=[ticker,e]\n",
    "        write_to_csv_last_line(csv_name,row)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "row=[round(elapsed_time, 3)]\n",
    "write_to_csv_last_line(csv_name,row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af603b11-fa2f-4bc0-b127-60e21c8e6f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
