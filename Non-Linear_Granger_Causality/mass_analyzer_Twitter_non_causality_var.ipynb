{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import nonlincausality as nlc\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "import ipynbname\n",
    "import time\n",
    "from numba import cuda\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# If you specifically want to ignore warnings from pandas, you can do so as follows\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pandas')\n",
    "\n",
    "\n",
    "# Set options to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Optionally, set the display width to ensure that pandas does not wrap text\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set the column display length to maximum\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########GPU CONFIG################\n",
    "'''\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpu_number = 0 #### GPU number \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[gpu_number], 'GPU') \n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "'''\n",
    "###########GPU CONFIG################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########GPU CONFIG################\n",
    "'''\n",
    "import tensorflow as tf\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU available: Yes\")\n",
    "else:\n",
    "    print(\"GPU available: No\")\n",
    "'''\n",
    "###########GPU CONFIG################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sciencedirect.com/science/article/pii/S0169260722000542?via%3Dihub\n",
    "def non_linear_granger_test(filename,data_train,data_val,data_test,lags):\n",
    "    result = nlc.nonlincausalityNN(\n",
    "    x=data_train,\n",
    "    maxlag=lags,\n",
    "    NN_config=['d','dr','d','dr'],\n",
    "    NN_neurons=[100,0.05,100,0.05],\n",
    "    x_test=data_test,\n",
    "    run=5,\n",
    "    epochs_num=[30,30],\n",
    "    learning_rate=[0.0001, 0.00001],\n",
    "    batch_size_num=32,\n",
    "    x_val=data_val,\n",
    "    reg_alpha=None,\n",
    "    callbacks=None,\n",
    "    verbose=False,\n",
    "    plot=False,\n",
    "    )\n",
    "    return result\n",
    "    \n",
    "\n",
    "def data_transformer(data):\n",
    "    \n",
    "    # Update the DataFrame's column names\n",
    "    new_columns = [col.replace(\" - Realtime\", \"\") for col in data.columns]\n",
    "    data.columns = new_columns\n",
    "    \n",
    "    for column in data.select_dtypes(include=['object']).columns:\n",
    "    # Replace \",\" with \".\" in the entire column and convert the column to float\n",
    "        if column != \"Date\":\n",
    "            data[column] = data[column].str.replace(\",\", \".\").astype(float)\n",
    "    \n",
    "    data = data.sort_values(by='Date')\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    #Make the modifications for the analysis\n",
    "    data = data.dropna(subset=[\"(R1) Close\", \"Twitter Positive Sentiment Count\", \"Twitter Negative Sentiment Count\"])\n",
    "    \n",
    "    data[\"Twitter Sentiment Count\"] = data[\"Twitter Positive Sentiment Count\"] - data[\"Twitter Negative Sentiment Count\"]    \n",
    "    #Drop the rows where the Twitter Sentiment Count is 0 because the % change is not calculable at these values\n",
    "    #data = data[data['Twitter Sentiment Count'] != 0]\n",
    "    #data['Twitter Sentiment Change'] = np.log(data['Twitter Sentiment Count'] / data['Twitter Sentiment Count'].shift(1))\n",
    "    #data[\"Daily Sentiment Variance\"] =data['Twitter Sentiment Change'] #np.absolute(data[\"Twitter Sentiment Change\"])\n",
    "    data['Twitter Sentiment Change'] = data['Twitter Positive Sentiment Count'] / data['Twitter Publication Count (L1)']\n",
    "    data[\"Daily Sentiment Variance\"] = data['Twitter Sentiment Change'] \n",
    "\n",
    "    \n",
    "    \n",
    "    data['Log Returns'] = np.log(data['(R1) Close'] / data['(R1) Close'].shift(1))\n",
    "    data['Log Returns Variance'] =data['Log Returns']**2\n",
    "    \n",
    "    \n",
    "    # data['Daily Sentiment Change'] = data['Twitter Sentiment Count'].pct_change()\n",
    "    \n",
    "    \n",
    "    #CHANGE!\n",
    "    #data['Daily Sentiment Change'] = data['Twitter Publication Count (L1)'].pct_change()\n",
    "    #data['Daily Sentiment Change'] =np.log(data['Twitter Publication Count (L1)'] / data['Twitter Publication Count (L1)'].shift(1))\n",
    "\n",
    "    #data['Daily Sentiment Variance'] = data['Daily Sentiment Change']**2\n",
    "    data = data.replace(np.inf, 0)\n",
    "    data = data.dropna(subset=[\"Daily Sentiment Variance\"])\n",
    "    data = data.dropna(subset=[\"Log Returns\"])\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def create_list_up_to_number(number):\n",
    "    return [i for i in range(1, number + 1)]\n",
    "\n",
    "def calculate_mse_for_lists(residuals):\n",
    "    mses = []\n",
    "    for inner_list in residuals:\n",
    "        mse = calculate_mse_from_residuals(inner_list)\n",
    "        mses.append(mse)\n",
    "    return mses\n",
    "    \n",
    "def calculate_mse_from_residuals(residuals):\n",
    "    squared_errors = [(residual) ** 2 for residual in residuals]\n",
    "    mse = sum(squared_errors) / len(residuals)\n",
    "    return mse\n",
    "    \n",
    "def calculate_mae_for_lists(residuals):\n",
    "    maes = []\n",
    "    for inner_list in residuals:\n",
    "        mae = calculate_mae_from_residuals(inner_list)\n",
    "        maes.append(mae)\n",
    "    return maes\n",
    "    \n",
    "def calculate_mae_from_residuals(residuals):\n",
    "    absolute_errors = [abs(residual) for residual in residuals]\n",
    "    mae = sum(absolute_errors) / len(residuals)\n",
    "    return mae\n",
    "    \n",
    "def calculate_rss_for_lists(residuals):\n",
    "    total_residuals = []\n",
    "    for inner_list in residuals:\n",
    "        total_residual = calculate_total_residuals(inner_list)\n",
    "        total_residuals.append(total_residual)\n",
    "    return total_residuals  \n",
    "    \n",
    "def calculate_total_residuals(residuals):\n",
    "    total_residuals = sum(residuals)\n",
    "    return total_residuals\n",
    "    \n",
    "def write_to_csv_last_line(csv_file_path, new_data):\n",
    "    # Open the CSV file in append mode with newline='' to handle new line characters correctly\n",
    "    with open(csv_file_path, 'a', newline='') as csv_file:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write data into the last new line\n",
    "        csv_writer.writerow(new_data)\n",
    "    \n",
    "    return None\n",
    "    \n",
    "def extend_lists(*lists):\n",
    "    combined_list = []\n",
    "    for lst in lists:\n",
    "        combined_list.extend(lst)\n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_data = pd.read_excel(\"S&P500.xlsx\")\n",
    "sp500 = sp500_data[\"Ticker\"]\n",
    "sp500 = sp500.str.replace(\"/\", \"_\")\n",
    "\n",
    "#Módosítás_1\n",
    "#Itt lehet beállítani a főbb paramétereket a program futtatásához\n",
    "\n",
    "#Nem lineáris lag paraméter\n",
    "n_lag=10\n",
    "lags = create_list_up_to_number(n_lag)\n",
    "\n",
    "#Az a szignifikancia szint, ami felett elfogadjuk a granger okságot\n",
    "threshold = 0.05\n",
    "\n",
    "# sp500 = sp500.head(5)\n",
    "data_folder = \"Twitter_Daily_5Y\"\n",
    "\n",
    "#nem lineáris kauzalitás vizsgálathoz paraméterek\n",
    "#a teljes adat mekkora része teszt\n",
    "test_treshold=0.3\n",
    "#a train adat mekkor része legyen validációs adat\n",
    "val_treshold=0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "## nem lineáris kauzalitás tesztelése részvényen\n",
    "cwd = Path.cwd()\n",
    "file_path = cwd / data_folder / 'AAPL UW Equity.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data_transformer(data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data=scaler.fit_transform(data[['Log Returns Variance', 'Daily Sentiment Variance']])\n",
    "data = pd.DataFrame(data, columns=['Log Returns Variance', 'Daily Sentiment Variance'])\n",
    "\n",
    "lags_test = lags\n",
    "train_val_df, test_df = train_test_split(data, test_size=test_treshold,shuffle=False) \n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_treshold,shuffle=False)\n",
    "\n",
    "result=non_linear_granger_test('TSLA',train_df[['Log Returns Variance', 'Daily Sentiment Variance']].values,val_df[['Log Returns Variance', 'Daily Sentiment Variance']].values,test_df[['Log Returns Variance', 'Daily Sentiment Variance']].values,lags_test)\n",
    "#result1=non_linear_granger_test('TSLA',train_df[[ 'Daily Sentiment Variance','Log Returns Variance']].values,val_df[[ 'Daily Sentiment Variance','Log Returns Variance']].values,test_df[[ 'Daily Sentiment Variance','Log Returns Variance']].values,lags_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "nb_fname = ipynbname.name()\n",
    "csv_name_1=nb_fname+\"_Sentiment.csv\"\n",
    "#csv_name_2=nb_fname+\"_Stock.csv\"\n",
    "\n",
    "#create csv with header\n",
    "header=[]\n",
    "header.append(\"Ticker\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"Test_statistics_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"p_value_{i}\")    \n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"X_in_sample_MSE_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"X_in_sample_MAE_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"X_in_sample_RSS_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"X_prediction_MSE_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"X_prediction_MAE_{i}\") \n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"X_prediction_RSS_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"XY_in_sample_MSE_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"XY_in_sample_MAE_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"XY_in_sample_RSS_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"XY_prediction_MSE_{i}\")\n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"XY_prediction_MAE_{i}\") \n",
    "for i in range(1, n_lag+1):\n",
    "    header.append(f\"XY_prediction_RSS_{i}\")\n",
    "#header\n",
    "\n",
    "\n",
    "csv_file = open(csv_name_1, 'w', newline='')\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "#csv_file1 = open(csv_name_2, 'w', newline='')\n",
    "#writer1 = csv.writer(csv_file1)\n",
    "\n",
    "writer.writerow(header)\n",
    "#writer1.writerow(header)\n",
    "\n",
    "csv_file.close()\n",
    "#csv_file1.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# Analyze the data and get results from Non Linear Granger\n",
    "cwd = Path.cwd()\n",
    "\n",
    "# File path to write the CSV data\n",
    "nb_fname = ipynbname.name()\n",
    "csv_name_1=nb_fname+\"_Sentiment.csv\"\n",
    "#csv_name_2=nb_fname+\"_Stock.csv\"\n",
    "\n",
    "for filename_raw in sp500:\n",
    "    filename = filename_raw + \".csv\"\n",
    "    file_path = cwd / data_folder / filename\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        short_ticker = filename_raw.split(\" \")[0]\n",
    "        print(f\"{short_ticker} loaded successfully.\")\n",
    "        data = data_transformer(data)\n",
    "\n",
    "        try:\n",
    "            scaler = StandardScaler()\n",
    "    \n",
    "            data=scaler.fit_transform(data[['Log Returns Variance', 'Daily Sentiment Variance']])\n",
    "            data = pd.DataFrame(data, columns=['Log Returns Variance', 'Daily Sentiment Variance'])\n",
    "            \n",
    "            # Analyze the data and get results from Non Linear Granger\n",
    "            train_val_df, test_df = train_test_split(data, test_size=test_treshold,shuffle=False) \n",
    "            train_df, val_df = train_test_split(train_val_df, test_size=val_treshold,shuffle=False) \n",
    "    \n",
    "            ##dictionary-t add vissza\n",
    "            result=non_linear_granger_test(short_ticker,train_df[['Log Returns Variance', 'Daily Sentiment Variance']].values,val_df[['Log Returns Variance', 'Daily Sentiment Variance']].values,test_df[['Log Returns Variance', 'Daily Sentiment Variance']].values,lags)\n",
    "            #result1=non_linear_granger_test(short_ticker,train_df[[ 'Daily Sentiment Variance','Log Returns Variance']].values,val_df[[ 'Daily Sentiment Variance','Log Returns Variance']].values,test_df[[ 'Daily Sentiment Variance','Log Returns Variance']].values,lags)\n",
    "\n",
    "            result = [value for value in result.values()]\n",
    "            \n",
    "            #device = cuda.get_current_device()\n",
    "            #device.reset()\n",
    "            #time.sleep(2.5)\n",
    "            \n",
    "            statistics = [inner_list[0] for inner_list in result]\n",
    "            p_values=[inner_list[1] for inner_list in result]\n",
    "            \n",
    "            X_in_sample_MSE=calculate_mse_for_lists([inner_list[2] for inner_list in result])\n",
    "            X_in_sample_MAE=calculate_mae_for_lists([inner_list[2] for inner_list in result])\n",
    "            X_in_sample_RSS=calculate_rss_for_lists([inner_list[2] for inner_list in result])\n",
    "            \n",
    "            X_prediction_MSE=calculate_mse_for_lists([inner_list[3] for inner_list in result])\n",
    "            X_prediction_MAE=calculate_mae_for_lists([inner_list[3] for inner_list in result])\n",
    "            X_prediction_RSS=calculate_rss_for_lists([inner_list[3] for inner_list in result])\n",
    "    \n",
    "            XY_in_sample_MSE=calculate_mse_for_lists([inner_list[4] for inner_list in result])\n",
    "            XY_in_sample_MAE=calculate_mae_for_lists([inner_list[4] for inner_list in result])\n",
    "            XY_in_sample_RSS=calculate_rss_for_lists([inner_list[4] for inner_list in result])\n",
    "            \n",
    "            XY_prediction_MSE=calculate_mse_for_lists([inner_list[5] for inner_list in result])                                       \n",
    "            XY_prediction_MAE=calculate_mae_for_lists([inner_list[5] for inner_list in result])\n",
    "            XY_prediction_RSS=calculate_rss_for_lists([inner_list[5] for inner_list in result])\n",
    "    \n",
    "            row=extend_lists([short_ticker],statistics,p_values,X_in_sample_MSE,X_in_sample_MAE,X_in_sample_RSS,\n",
    "                              X_prediction_MSE,X_prediction_MAE,X_prediction_RSS,\n",
    "                              XY_in_sample_MSE,XY_in_sample_MAE,XY_in_sample_RSS,\n",
    "                              XY_prediction_MSE,XY_prediction_MAE,XY_prediction_RSS)\n",
    "\n",
    "            '''\n",
    "            result1 = [value for value in result1.values()]\n",
    "\n",
    "            statistics1 = [inner_list[0] for inner_list in result1]\n",
    "            p_values1=[inner_list[1] for inner_list in result1]\n",
    "            \n",
    "            X_in_sample_MSE1=calculate_mse_for_lists([inner_list[2] for inner_list in result1])\n",
    "            X_in_sample_MAE1=calculate_mae_for_lists([inner_list[2] for inner_list in result1])\n",
    "            X_in_sample_RSS1=calculate_rss_for_lists([inner_list[2] for inner_list in result1])\n",
    "            \n",
    "            X_prediction_MSE1=calculate_mse_for_lists([inner_list[3] for inner_list in result1])\n",
    "            X_prediction_MAE1=calculate_mae_for_lists([inner_list[3] for inner_list in result1])\n",
    "            X_prediction_RSS1=calculate_rss_for_lists([inner_list[3] for inner_list in result1])\n",
    "    \n",
    "            XY_in_sample_MSE1=calculate_mse_for_lists([inner_list[4] for inner_list in result1])\n",
    "            XY_in_sample_MAE1=calculate_mae_for_lists([inner_list[4] for inner_list in result1])\n",
    "            XY_in_sample_RSS1=calculate_rss_for_lists([inner_list[4] for inner_list in result1])\n",
    "            \n",
    "            XY_prediction_MSE1=calculate_mse_for_lists([inner_list[5] for inner_list in result1])                                        \n",
    "            XY_prediction_MAE1=calculate_mae_for_lists([inner_list[5] for inner_list in result1])\n",
    "            XY_prediction_RSS1=calculate_rss_for_lists([inner_list[5] for inner_list in result1])\n",
    "    \n",
    "            row1=extend_lists([short_ticker],statistics1,p_values1,X_in_sample_MSE1,X_in_sample_MAE1,X_in_sample_RSS1,\n",
    "                              X_prediction_MSE1,X_prediction_MAE1,X_prediction_RSS1,\n",
    "                              XY_in_sample_MSE1,XY_in_sample_MAE1,XY_in_sample_RSS1,\n",
    "                              XY_prediction_MSE1,XY_prediction_MAE1,XY_prediction_RSS1)\n",
    "            '''\n",
    "\n",
    "        except Exception as e:\n",
    "            row=[short_ticker,e]\n",
    "            row1=[short_ticker, e]\n",
    "        try:\n",
    "            write_to_csv_last_line(csv_name_1,row)\n",
    "            del row\n",
    "            del result\n",
    "            del data\n",
    "            del train_val_df, test_df,train_df, val_df\n",
    "            del statistics,p_values,X_in_sample_MSE,X_in_sample_MAE,X_in_sample_RSS,X_prediction_MSE,X_prediction_MAE,X_prediction_RSS,XY_in_sample_MSE,XY_in_sample_MAE,XY_in_sample_RSS,XY_prediction_MSE,XY_prediction_MAE,XY_prediction_RSS\n",
    "            del short_ticker\n",
    "            tf.keras.backend.clear_session()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        #write_to_csv_last_line(csv_name_2,row1)             \n",
    "    else:\n",
    "        print(f\"File '{filename}' does not exist in the {data_folder} folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
